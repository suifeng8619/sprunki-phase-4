#!/usr/bin/env python3
"""
é«˜çº§æ•°æ®åº“ä¼˜åŒ–ç³»ç»Ÿ
æä¾›æ›´æ·±å±‚çš„æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å’ŒæŸ¥è¯¢é‡æ„
"""

import time
from mongoengine import connect, Document
from loguru import logger
from apps.models.article_model import æ–‡ç« db
from cache_system import cached_function, article_cache

class AdvancedDatabaseOptimizer:
    """
    é«˜çº§æ•°æ®åº“ä¼˜åŒ–å™¨
    """
    
    def __init__(self):
        self.optimization_stats = {
            'queries_optimized': 0,
            'time_saved': 0,
            'cache_hits': 0
        }
    
    def create_advanced_indexes(self):
        """åˆ›å»ºé«˜çº§å¤åˆç´¢å¼•"""
        logger.info("ğŸ” åˆ›å»ºé«˜çº§æ•°æ®åº“ç´¢å¼•...")
        
        try:
            collection = æ–‡ç« db._get_collection()
            
            # å¤åˆç´¢å¼•é›†åˆ
            advanced_indexes = [
                # è¯­è¨€ + çŠ¶æ€ + å‘å¸ƒæ—¶é—´ (ç”¨äºå¤šè¯­è¨€é¦–é¡µæŸ¥è¯¢)
                {
                    'index': [("lang", 1), ("çŠ¶æ€", 1), ("å‘å¸ƒæ—¶é—´", -1)],
                    'name': "idx_lang_status_time",
                    'background': True
                },
                
                # æ–‡ç« URL + è¯­è¨€ (ç”¨äºURLè·¯ç”±æŸ¥è¯¢)
                {
                    'index': [("article_url", 1), ("lang", 1)],
                    'name': "idx_url_lang",
                    'background': True
                },
                
                # åˆ†ç±» + çŠ¶æ€ + æ—¶é—´ (ç”¨äºåˆ†ç±»é¡µé¢)
                {
                    'index': [("åˆ†ç±»", 1), ("çŠ¶æ€", 1), ("å‘å¸ƒæ—¶é—´", -1)],
                    'name': "idx_category_status_time", 
                    'background': True
                },
                
                # æ ‡ç­¾æ•°ç»„ç´¢å¼• (ç”¨äºæ ‡ç­¾æŸ¥è¯¢)
                {
                    'index': [("æ ‡ç­¾", 1), ("çŠ¶æ€", 1)],
                    'name': "idx_tags_status",
                    'background': True
                },
                
                # æ–‡æœ¬æœç´¢ç´¢å¼•
                {
                    'index': [("æ ‡é¢˜", "text"), ("ç®€ä»‹", "text"), ("æ­£æ–‡å†…å®¹", "text")],
                    'name': "idx_fulltext_search",
                    'background': True
                }
            ]
            
            created_count = 0
            for idx_config in advanced_indexes:
                try:
                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
                    existing_indexes = collection.list_indexes()
                    index_exists = any(idx.get('name') == idx_config['name'] for idx in existing_indexes)
                    
                    if not index_exists:
                        collection.create_index(
                            idx_config['index'],
                            name=idx_config['name'],
                            background=idx_config['background']
                        )
                        created_count += 1
                        logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {idx_config['name']}")
                    else:
                        logger.debug(f"â© ç´¢å¼•å·²å­˜åœ¨: {idx_config['name']}")
                        
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥ {idx_config['name']}: {e}")
            
            logger.info(f"ğŸ¯ é«˜çº§ç´¢å¼•åˆ›å»ºå®Œæˆ: {created_count} ä¸ªæ–°ç´¢å¼•")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é«˜çº§ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    @cached_function(cache_instance=article_cache, timeout=900)
    def get_articles_by_language_optimized(self, lang='en', limit=20, skip=0, status='å‘å¸ƒ'):
        """
        ä¼˜åŒ–çš„è¯­è¨€æ–‡ç« æŸ¥è¯¢
        ä½¿ç”¨å¤åˆç´¢å¼•å’ŒæŠ•å½±ä¼˜åŒ–
        """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨å¤åˆç´¢å¼•: lang + çŠ¶æ€ + å‘å¸ƒæ—¶é—´
            articles = æ–‡ç« db.objects(
                lang=lang,
                çŠ¶æ€=status
            ).only(
                'ids',
                'æ ‡é¢˜', 
                'ç®€ä»‹',
                'article_url',
                'image_url',
                'å‘å¸ƒæ—¶é—´'
            ).order_by('-å‘å¸ƒæ—¶é—´').skip(skip).limit(limit)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨ä»¥å‡å°‘åºåˆ—åŒ–å¼€é”€
            result = []
            for article in articles:
                result.append({
                    'ids': article.ids,
                    'title': article.æ ‡é¢˜,
                    'summary': article.ç®€ä»‹,
                    'url': article.article_url,
                    'image': article.image_url,
                    'publish_time': article.å‘å¸ƒæ—¶é—´.isoformat() if article.å‘å¸ƒæ—¶é—´ else None
                })
            
            query_time = (time.time() - start_time) * 1000
            logger.info(f"è¯­è¨€æ–‡ç« æŸ¥è¯¢ä¼˜åŒ–: {lang}, {len(result)}æ¡, è€—æ—¶: {query_time:.2f}ms")
            
            self.optimization_stats['queries_optimized'] += 1
            self.optimization_stats['time_saved'] += max(0, 1000 - query_time)  # å‡è®¾åŸæŸ¥è¯¢éœ€1ç§’
            
            return result
            
        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            logger.error(f"è¯­è¨€æ–‡ç« æŸ¥è¯¢å¤±è´¥: {lang}, è€—æ—¶: {query_time:.2f}ms, é”™è¯¯: {e}")
            return []
    
    @cached_function(cache_instance=article_cache, timeout=1200)  
    def get_article_by_url_advanced(self, article_url, lang='en'):
        """
        é«˜çº§URLæ–‡ç« æŸ¥è¯¢
        ä½¿ç”¨å¤åˆç´¢å¼•ä¼˜åŒ–
        """
        start_time = time.time()
        
        try:
            # ä½¿ç”¨å¤åˆç´¢å¼•: article_url + lang
            article = æ–‡ç« db.objects(
                article_url=article_url,
                lang=lang,
                çŠ¶æ€='å‘å¸ƒ'
            ).only(
                'ids',
                'æ ‡é¢˜',
                'ç®€ä»‹', 
                'iframe',
                'image_url',
                'æ­£æ–‡å†…å®¹',
                'article_url',
                'å‘å¸ƒæ—¶é—´',
                'æ ‡ç­¾'
            ).first()
            
            if not article:
                return None
            
            # ä¼˜åŒ–çš„æ•°æ®ç»“æ„
            result = {
                'ids': article.ids,
                'title': article.æ ‡é¢˜,
                'summary': article.ç®€ä»‹,
                'content': article.æ­£æ–‡å†…å®¹,
                'iframe': article.iframe,
                'image_url': article.image_url,
                'article_url': article.article_url,
                'publish_time': article.å‘å¸ƒæ—¶é—´.isoformat() if article.å‘å¸ƒæ—¶é—´ else None,
                'tags': article.æ ‡ç­¾ if article.æ ‡ç­¾ else []
            }
            
            query_time = (time.time() - start_time) * 1000
            logger.info(f"URLæ–‡ç« æŸ¥è¯¢ä¼˜åŒ–: {article_url}, è€—æ—¶: {query_time:.2f}ms")
            
            self.optimization_stats['queries_optimized'] += 1
            return result
            
        except Exception as e:
            query_time = (time.time() - start_time) * 1000
            logger.error(f"URLæ–‡ç« æŸ¥è¯¢å¤±è´¥: {article_url}, è€—æ—¶: {query_time:.2f}ms, é”™è¯¯: {e}")
            return None
    
    @cached_function(cache_instance=article_cache, timeout=600)
    def get_related_articles(self, article_id, lang='en', limit=5):
        """
        è·å–ç›¸å…³æ–‡ç« ï¼ˆåŸºäºæ ‡ç­¾å’Œåˆ†ç±»ï¼‰
        """
        start_time = time.time()
        
        try:
            # é¦–å…ˆè·å–å½“å‰æ–‡ç« çš„æ ‡ç­¾å’Œåˆ†ç±»
            current_article = æ–‡ç« db.objects(ids=article_id).only('æ ‡ç­¾', 'åˆ†ç±»').first()
            if not current_article:
                return []
            
            # æŸ¥è¯¢æ¡ä»¶ï¼šç›¸åŒæ ‡ç­¾æˆ–åˆ†ç±»ï¼Œæ’é™¤å½“å‰æ–‡ç« 
            query_conditions = {
                'lang': lang,
                'çŠ¶æ€': 'å‘å¸ƒ',
                'ids__ne': article_id  # æ’é™¤å½“å‰æ–‡ç« 
            }
            
            # æ·»åŠ æ ‡ç­¾æˆ–åˆ†ç±»åŒ¹é…æ¡ä»¶
            or_conditions = []
            if current_article.æ ‡ç­¾:
                or_conditions.append({'æ ‡ç­¾__in': current_article.æ ‡ç­¾})
            if current_article.åˆ†ç±»:
                or_conditions.append({'åˆ†ç±»': current_article.åˆ†ç±»})
            
            if not or_conditions:
                return []
            
            # ä½¿ç”¨$oræŸ¥è¯¢ç›¸å…³æ–‡ç« 
            related_articles = æ–‡ç« db.objects(**query_conditions).filter(
                __raw__={'$or': [cond for cond in or_conditions]}
            ).only(
                'ids',
                'æ ‡é¢˜',
                'ç®€ä»‹',
                'article_url',
                'image_url'
            ).limit(limit)
            
            result = []
            for article in related_articles:
                result.append({
                    'ids': article.ids,
                    'title': article.æ ‡é¢˜,
                    'summary': article.ç®€ä»‹,
                    'url': article.article_url,
                    'image': article.image_url
                })
            
            query_time = (time.time() - start_time) * 1000
            logger.info(f"ç›¸å…³æ–‡ç« æŸ¥è¯¢: ID={article_id}, {len(result)}æ¡, è€—æ—¶: {query_time:.2f}ms")
            
            return result
            
        except Exception as e:
            query_time = (time.time() - start_time) * 1000  
            logger.error(f"ç›¸å…³æ–‡ç« æŸ¥è¯¢å¤±è´¥: ID={article_id}, è€—æ—¶: {query_time:.2f}ms, é”™è¯¯: {e}")
            return []
    
    def analyze_slow_queries(self):
        """åˆ†ææ…¢æŸ¥è¯¢"""
        logger.info("ğŸ” åˆ†ææ•°æ®åº“æ€§èƒ½...")
        
        try:
            collection = æ–‡ç« db._get_collection()
            
            # è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯
            stats = collection.database.command("collStats", collection.name)
            
            analysis = {
                'collection_size': stats.get('size', 0),
                'document_count': stats.get('count', 0),
                'index_count': len(list(collection.list_indexes())),
                'avg_document_size': stats.get('avgObjSize', 0)
            }
            
            logger.info(f"ğŸ“Š æ•°æ®åº“åˆ†æç»“æœ:")
            logger.info(f"   æ–‡æ¡£æ•°é‡: {analysis['document_count']:,}")
            logger.info(f"   é›†åˆå¤§å°: {analysis['collection_size']:,} bytes")
            logger.info(f"   ç´¢å¼•æ•°é‡: {analysis['index_count']}")
            logger.info(f"   å¹³å‡æ–‡æ¡£å¤§å°: {analysis['avg_document_size']} bytes")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆ†æå¤±è´¥: {e}")
            return {}
    
    def get_optimization_stats(self):
        """è·å–ä¼˜åŒ–ç»Ÿè®¡"""
        return {
            **self.optimization_stats,
            'cache_stats': article_cache.get_stats()
        }

# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
db_optimizer = AdvancedDatabaseOptimizer()

def optimize_database_advanced():
    """æ‰§è¡Œé«˜çº§æ•°æ®åº“ä¼˜åŒ–"""
    logger.info("ğŸš€ å¼€å§‹é«˜çº§æ•°æ®åº“ä¼˜åŒ–...")
    
    # åˆ›å»ºé«˜çº§ç´¢å¼•
    success = db_optimizer.create_advanced_indexes()
    
    if success:
        # åˆ†ææ€§èƒ½
        db_optimizer.analyze_slow_queries()
        
        # æ˜¾ç¤ºä¼˜åŒ–ç»Ÿè®¡
        stats = db_optimizer.get_optimization_stats()
        logger.info(f"âœ… é«˜çº§ä¼˜åŒ–å®Œæˆ")
        logger.info(f"   ä¼˜åŒ–æŸ¥è¯¢æ•°: {stats['queries_optimized']}")
        logger.info(f"   èŠ‚çœæ—¶é—´: {stats['time_saved']:.2f}ms")
        
        return True
    else:
        logger.error("âŒ é«˜çº§æ•°æ®åº“ä¼˜åŒ–å¤±è´¥")
        return False

if __name__ == "__main__":
    # æ‰§è¡Œé«˜çº§æ•°æ®åº“ä¼˜åŒ–
    print("ğŸš€ å¼€å§‹é«˜çº§æ•°æ®åº“ä¼˜åŒ–...")
    
    # è¿æ¥æ•°æ®åº“ (ä½¿ç”¨åº”ç”¨çš„æ•°æ®åº“é…ç½®)
    try:
        from setting import DATABASE_CONFIG
        if DATABASE_CONFIG.get('MONGO_URI'):
            connect(host=DATABASE_CONFIG['MONGO_URI'])
        else:
            connect('sprunki_test')  # é»˜è®¤æµ‹è¯•æ•°æ®åº“
        
        # æ‰§è¡Œä¼˜åŒ–
        success = optimize_database_advanced()
        
        if success:
            print("âœ… é«˜çº§æ•°æ®åº“ä¼˜åŒ–å®Œæˆï¼")
            
            # æµ‹è¯•ä¼˜åŒ–åçš„æŸ¥è¯¢
            print("\nğŸ§ª æµ‹è¯•ä¼˜åŒ–æŸ¥è¯¢...")
            optimizer = AdvancedDatabaseOptimizer()
            
            # æµ‹è¯•è¯­è¨€æ–‡ç« æŸ¥è¯¢
            articles = optimizer.get_articles_by_language_optimized('en', limit=5)
            print(f"âœ… è‹±è¯­æ–‡ç« æŸ¥è¯¢: {len(articles)} æ¡ç»“æœ")
            
            # æµ‹è¯•URLæŸ¥è¯¢
            if articles:
                article_url = articles[0].get('url')
                if article_url:
                    article = optimizer.get_article_by_url_advanced(article_url, 'en')
                    print(f"âœ… URLæ–‡ç« æŸ¥è¯¢: {'æˆåŠŸ' if article else 'æœªæ‰¾åˆ°'}")
            
            print("ğŸ¯ é«˜çº§ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        else:
            print("âŒ é«˜çº§æ•°æ®åº“ä¼˜åŒ–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {e}")